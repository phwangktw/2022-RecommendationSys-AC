{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session2_Content-based_Recommendation_Algorithm.ipynb.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/phwangktw/data-course-sample/blob/main/Session2_Content_based_Recommendation_Algorithm_ipynb.ipynb",
      "authorship_tag": "ABX9TyP13BBOiFTdxRiL/EyxHK/M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phwangktw/data-course-sample/blob/main/Session3_Collaborative-based(user-based)_Recommendation_Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Session3: Collaborative-based(user-based)_Recommendation_Algorithm.ipynb"
      ],
      "metadata": {
        "id": "Yz-rghKTyqwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step1. Packages import and utiltiets functions\n",
        "\n",
        "*   `nltk` package used for text parsing.\n",
        "\n"
      ],
      "metadata": {
        "id": "h7dsDP9Xyv4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hai9bqC5q1SL",
        "outputId": "98b7be5b-3ff6-4561-ea88-98509dd8d48d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7UjSP35yo4j",
        "outputId": "e7728e4b-4613-42f3-8ec7-dfe041f37e2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gzip, json\n",
        "from os.path import exists\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import datetime\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "def parse(path):\n",
        "    g = gzip.open(path, 'rb')\n",
        "    for l in g:\n",
        "        yield json.loads(l)\n",
        "\n",
        "def getDF(path):\n",
        "    i = 0\n",
        "    df = {}\n",
        "    for d in parse(path):\n",
        "        df[i] = d\n",
        "        i += 1\n",
        "    return pd.DataFrame.from_dict(df, orient='index')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "def content_filter(text):\n",
        "    # stopwords = nltk.corpus.stopwords.words('english')\n",
        "    content = [w for w in text if (w.lower() not in stop_words) & (w.isalnum()) ]\n",
        "    return content"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step2. Download data"
      ],
      "metadata": {
        "id": "WhNHwYPky2V_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/All_Beauty.csv\n",
        "!wget http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles2/meta_All_Beauty.json.gz\n",
        "\n",
        "metadata = getDF('/content/meta_All_Beauty.json.gz')\n",
        "ratings = pd.read_csv('/content/All_Beauty.csv', names=['asin', 'reviewerID', 'overall', 'unixReviewTime'], header=None)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrM-TOz6y2nx",
        "outputId": "2d8332da-bb1d-4304-a66b-921316bdc473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-03 20:30:49--  http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/All_Beauty.csv\n",
            "Resolving deepyeti.ucsd.edu (deepyeti.ucsd.edu)... 169.228.63.50\n",
            "Connecting to deepyeti.ucsd.edu (deepyeti.ucsd.edu)|169.228.63.50|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15499476 (15M) [application/octet-stream]\n",
            "Saving to: ‘All_Beauty.csv’\n",
            "\n",
            "All_Beauty.csv      100%[===================>]  14.78M  8.16MB/s    in 1.8s    \n",
            "\n",
            "2022-01-03 20:30:51 (8.16 MB/s) - ‘All_Beauty.csv’ saved [15499476/15499476]\n",
            "\n",
            "--2022-01-03 20:30:51--  http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles2/meta_All_Beauty.json.gz\n",
            "Resolving deepyeti.ucsd.edu (deepyeti.ucsd.edu)... 169.228.63.50\n",
            "Connecting to deepyeti.ucsd.edu (deepyeti.ucsd.edu)|169.228.63.50|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10329961 (9.9M) [application/octet-stream]\n",
            "Saving to: ‘meta_All_Beauty.json.gz’\n",
            "\n",
            "meta_All_Beauty.jso 100%[===================>]   9.85M  6.02MB/s    in 1.6s    \n",
            "\n",
            "2022-01-03 20:30:53 (6.02 MB/s) - ‘meta_All_Beauty.json.gz’ saved [10329961/10329961]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step3. Parsing data"
      ],
      "metadata": {
        "id": "cyt6lpHCzKu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step3-1: Convert time format"
      ],
      "metadata": {
        "id": "234dDW6TzOnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratings['DATE'] = pd.to_datetime(ratings['unixReviewTime'], unit='s')"
      ],
      "metadata": {
        "id": "R2gHgYvNzLAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step3-2: Data preprocessing\n",
        "(As same as [session1](https://github.com/phwangktw/data-course-sample/blob/main/Session1_Rule-based_Recommendation_Algorithm.ipynb))\n",
        "\n",
        "*   Dropout the duplicated rows\n",
        "*   Fill the blanks with `nan`\n",
        "*   Parsing the `description` column for generating `rank_num` and `rank_category`\n",
        "*   Regex expression for searching specific key words"
      ],
      "metadata": {
        "id": "hQepuRY9zTAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Cleaning data (cited from: https://github.com/yuchiahung/data-course-sample/blob/main/hw1_Ana.ipynb)\n",
        "##Peaking data firstly\n",
        "metadata_clean = metadata.loc[metadata.astype(str).drop_duplicates().index]\n",
        "metadata_clean.replace('', np.nan, inplace = True)\n",
        "\n",
        "\n",
        "# clean column `rank` -> Parsing out to RankNum + RankCategory\n",
        "metadata_clean['rank'] = metadata_clean['rank'].str.replace('&amp;', '&')\n",
        "metadata_clean['rank'].fillna('0', inplace = True)\n",
        "metadata_clean['rank_category'] = [re.search('in (.*) \\(', r).group(1) if r != '0' else None for r in metadata_clean['rank']]\n",
        "metadata_clean['rank_num'] = [re.search('(.*) in .*', r).group(1) if r != '0' else None for r in metadata_clean['rank']]\n",
        "metadata_clean['rank_num'] = metadata_clean['rank_num'].str.replace(',', '').astype(float)\n",
        "\n",
        "# excluding category != 'Beauty & Personal Care'\n",
        "metadata_clean = metadata_clean[metadata_clean.rank_category == 'Beauty & Personal Care']\n",
        "\n",
        "# convert `price` to float\n",
        "metadata_clean['price'].fillna('0', inplace = True)\n",
        "metadata_clean['price'] = [re.search('\\$(.*)', p).group(1) if re.search('\\$(.*)', p) != None else None for p in metadata_clean['price']]\n",
        "metadata_clean['price'] = metadata_clean['price'].str.replace(',', '').astype(float)\n",
        "\n",
        "# drop useless columns\n",
        "metadata_clean.drop(\n",
        "    ['category', 'tech1', 'fit', 'tech2', 'date', 'similar_item', 'feature', 'main_cat', 'rank'], \n",
        "    axis = 1, \n",
        "    inplace = True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If7B5QErzLP6",
        "outputId": "24d20ce9-03f6-4391-a0a0-20fa6f867a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/series.py:4536: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  downcast=downcast,\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step3-3: Analyze the distribution of the words\n",
        "*   Problem statement:\n",
        "\n",
        "The original texts in the `title` contain many redundant words such as *a the, are, not* ...etc, which will unavoidably raise the matrix/vector size in the calculation of `similarity_matrix` and the `tokenization`.\n",
        "*   Solution:\n",
        "`nltk` package is used to do:\n",
        "\n",
        "\n",
        "1.   Filter out the stopwords: `content_filter(text)`\n",
        "2.   Select words with occurance >1 to sufficiently represent the original text. (`N=15904`)"
      ],
      "metadata": {
        "id": "RSon2t4KGoU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step4-2: Split time frame for testing and validation purpose"
      ],
      "metadata": {
        "id": "haPjtQk3zaRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_trainings = ratings[\n",
        "    (ratings['DATE'] < '2018-09-01')\n",
        "]\n",
        "ratings_testings = ratings[\n",
        "    (ratings['DATE'] >= '2018-09-01') & \n",
        "    (ratings['DATE'] <= '2018-09-30')\n",
        "]\n",
        "ratings_testings_by_user = ratings_testings.groupby('reviewerID').agg(list).reset_index()[['reviewerID', 'asin']].to_dict('records')\n",
        "ratings_testings_by_user = { rating['reviewerID']: rating['asin'] for rating in ratings_testings_by_user }\n",
        "users = list(ratings_testings_by_user.keys())"
      ],
      "metadata": {
        "id": "Z_EvEzeszLfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "\n",
        "# header: user_id,item_id,rating,timestamp\n",
        "\n",
        "def recommender(training_data, users=[], k=10):\n",
        "\n",
        "    # loading data from dataframe\n",
        "    # user_to_items dict:\n",
        "    # {\n",
        "    #   'user': {\n",
        "    #       'item': ratings...\n",
        "    #   }...\n",
        "    # }\n",
        "    user_to_items = defaultdict(dict)\n",
        "    for _, row in training_data.iterrows():\n",
        "        row = dict(row)\n",
        "        user = row['reviewerID']\n",
        "        item = row['asin']\n",
        "        rating = float(row['overall'])\n",
        "\n",
        "        user_to_items[user][item] = rating\n",
        "\n",
        "    print(\"total users before filtering: \", len(user_to_items))\n",
        "\n",
        "    # remove obscure user to decrease data size\n",
        "    # filtering params\n",
        "    remove_obscure_user = True\n",
        "    user_rating_threshold = 3\n",
        "    all_users = list(user_to_items.keys())\n",
        "    for user in all_users:\n",
        "        ratings = user_to_items[user]\n",
        "        if remove_obscure_user and len(ratings) < user_rating_threshold:\n",
        "            del user_to_items[user]\n",
        "\n",
        "    print(\"total users  after filtering: \", len(user_to_items))\n",
        "\n",
        "    # generate item to user mapping dict\n",
        "    # {\n",
        "    #   'item': {\n",
        "    #       'user': ratings...\n",
        "    #   }...\n",
        "    # }\n",
        "    item_to_users = defaultdict(dict)\n",
        "    for user, items in user_to_items.items():\n",
        "        for item, rating in items.items():\n",
        "            item_to_users[item][user] = rating\n",
        "\n",
        "    # prepare data of computing user similarity \n",
        "    init_sim = lambda: [0 for _ in range(3)]\n",
        "    factory = lambda: defaultdict(init_sim)\n",
        "    pre_user_similarity = defaultdict(factory)\n",
        "    n = len(item_to_users)\n",
        "    index = 0\n",
        "    for item, user_ratings in item_to_users.items():\n",
        "        if len(user_ratings) > 1:\n",
        "            # print(f\"item: {item} have been rated by {len(user_ratings)} users progress: {index}/{n}\")\n",
        "            for user1, user2 in combinations(user_ratings.keys(), 2):\n",
        "                xy = user_ratings[user1] * user_ratings[user2]\n",
        "                xx = user_ratings[user1] ** 2\n",
        "                yy = user_ratings[user2] ** 2\n",
        "                pre_user_similarity[user1][user2][0] += xy\n",
        "                pre_user_similarity[user1][user2][1] += xx\n",
        "                pre_user_similarity[user1][user2][2] += yy\n",
        "\n",
        "                pre_user_similarity[user2][user1][0] += xy\n",
        "                pre_user_similarity[user2][user1][1] += xx\n",
        "                pre_user_similarity[user2][user1][2] += yy\n",
        "        index += 1\n",
        "\n",
        "    user_similarity = {}\n",
        "    for src_user in pre_user_similarity:\n",
        "        user_similarity_order = []\n",
        "        for dst_user, val in pre_user_similarity[src_user].items():\n",
        "            xy = val[0]\n",
        "            xx = val[1]\n",
        "            yy = val[2]\n",
        "            div = ((xx*yy) ** 0.5)\n",
        "            if div == 0:\n",
        "                continue\n",
        "            similarity = xy / div\n",
        "            if similarity < 0:\n",
        "                continue\n",
        "            for i, s in enumerate(user_similarity_order):\n",
        "                target_similarity = s[1]\n",
        "                if target_similarity < similarity:\n",
        "                    user_similarity_order.insert(i, (dst_user, similarity))\n",
        "                    break\n",
        "            else:\n",
        "                user_similarity_order.append((dst_user, similarity))\n",
        "        user_similarity[src_user] = user_similarity_order\n",
        "\n",
        "    recommendation = {}\n",
        "    for user in users:\n",
        "        if user in user_similarity:\n",
        "            sim_users = user_similarity[user]\n",
        "            recommended_items = []\n",
        "            recommended_items_set = set()\n",
        "            user_have_rated = set(user_to_items[user])\n",
        "            stop_recommend = False\n",
        "            for sim_user, _ in sim_users:\n",
        "                items_from_sim_user = sorted(list(user_to_items[sim_user].items()), key=lambda item: item[1])\n",
        "                for item, _ in items_from_sim_user:\n",
        "                    if item not in user_have_rated and item not in recommended_items_set:\n",
        "                        recommended_items.append(item)\n",
        "                        recommended_items_set.add(item)\n",
        "                    if len(recommended_items) >= k:\n",
        "                        stop_recommend = True\n",
        "                        break\n",
        "                if stop_recommend:\n",
        "                    break\n",
        "            recommendation[user] = recommended_items\n",
        "        else:\n",
        "            recommendation[user] = []\n",
        "    return recommendation\n",
        "\n",
        "ratings_by_user = recommender(ratings_trainings, users)\n",
        "ratings_by_user"
      ],
      "metadata": {
        "id": "qK_Crvs--m39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step4-3: Content-based Model setup \n",
        "\n",
        "\n",
        "*   Stpe1: Set a fixed number (`k1=5`) for the content-based recommendation, and others for the rule-based recommendation.\n",
        "*   Step2: Examine the user whether or not the history purchases exist.\n",
        "*   Step3: Combine both content-based and rule-based results as the users' recommendations.\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "3C5DACT8P1YK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recommender_content1(training_data, users=[], k=10):\n",
        "    '''\n",
        "    * training_data: dataframe 輸入的訓練資料集（2018-09-01 以前資料）\n",
        "    * users: [] 需要被推薦的使用者\n",
        "    * k: int 每個使用者需要推薦的商品數\n",
        "    * recommendations: dict\n",
        "      {\n",
        "          使用者一： [推薦商品一, 推薦商品二, ...],\n",
        "          使用者二： [...], ...\n",
        "      }\n",
        "    '''\n",
        "    recommendations = {}\n",
        "       \n",
        "    for user in users:\n",
        "        content_recom = []\n",
        "        rule_recom = []\n",
        "        k1 = 5\n",
        "        ### UserID convert to purchase history\n",
        "        ## Content-based \n",
        "        ### Ensure the user has existing purchase (comment) history\n",
        "        existHistory = metadata[metadata['asin'].isin(ratings_trainings[ratings_trainings['reviewerID'] == user]['asin'].tolist())]['asin'].tolist()\n",
        "        if len(existHistory) > 0:\n",
        "            content_recom = recommend_items(existHistory,k1)\n",
        "        \n",
        "        # Popular products (recommend `k_left` products)\n",
        "        k_left = k - len(content_recom)\n",
        "        ## Best seller (by rating data) & highest rating products (recommend `k` product)\n",
        "        products_rating = training_data[training_data.DATE >= '2017-09-01'].groupby('asin')[['overall']].agg(['mean', 'count'])\n",
        "        products_rating.columns = products_rating.columns.droplevel(0)\n",
        "        rule_recom = products_rating.sort_values(by = ['count', 'mean'], ascending = False).index.tolist()[:k_left]\n",
        "        \n",
        "        \n",
        "        # concat all the item lists (k2 by rank, k3 by rating, others by sales)\n",
        "        user_recom = content_recom + rule_recom\n",
        "        recommendations[user] = user_recom\n",
        "\n",
        "    return recommendations"
      ],
      "metadata": {
        "id": "dXFFg_xtIgj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step4-4: Base Model setup \n",
        "Base case setup as the rule-based algorithm of the most K popular products of the recent year (see as Session1). "
      ],
      "metadata": {
        "id": "P_JbJOsuPUpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Rule1: A year-based recommendation\n",
        "def recommender_base(training_data, users=[], k=10):\n",
        "    '''\n",
        "    * training_data: dataframe 輸入的訓練資料集（2018-09-01 以前資料）\n",
        "    * users: [] 需要被推薦的使用者\n",
        "    * k: int 每個使用者需要推薦的商品數\n",
        "    * recommendations: dict\n",
        "      {\n",
        "          使用者一： [推薦商品一, 推薦商品二, ...],\n",
        "          使用者二： [...], ...\n",
        "      }\n",
        "    '''\n",
        "    recommendations = {}\n",
        "    ## Best seller (by rating data) & highest rating products (recommend `k` product)\n",
        "    products_rating = training_data[training_data.DATE >= '2017-09-01'].groupby('asin')[['overall']].agg(['mean', 'count'])\n",
        "    products_rating.columns = products_rating.columns.droplevel(0)\n",
        "    best_seller_lst = products_rating.sort_values(by = ['count', 'mean'], ascending = False).index.tolist()[:k]\n",
        "\n",
        "    recommendations = {user: best_seller_lst for user in users}\n",
        "    return recommendations"
      ],
      "metadata": {
        "id": "Jy_4X86r9J8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step4-5: Evaluation Algorithm and the Results"
      ],
      "metadata": {
        "id": "aboqSGJfVBuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(ratings_testings_by_user={}, ratings_by_user={}, method=None):\n",
        "    '''\n",
        "    * ratings_testings_by_user: dict 真實被購買的商品資料（2018-09-01 以後資料）\n",
        "    * ratings_by_user: dict 利用訓練資料學習的推薦商品\n",
        "    * method: str\n",
        "    * score: float\n",
        "    '''\n",
        "    total = 0\n",
        "    for d in ratings_testings_by_user:\n",
        "        if d in ratings_by_user:\n",
        "            total += len(set(ratings_by_user[d]) & set(ratings_testings_by_user[d]))\n",
        "\n",
        "    score = total / len(ratings_testings)\n",
        "    return score\n",
        "\n",
        "rcListRule1 = recommender_content1(ratings_trainings, users)\n",
        "rcListBase = recommender_base(ratings_trainings, users)\n",
        "\n",
        "scoreContent = evaluate(ratings_testings_by_user, rcListRule1)\n",
        "scoreBase = evaluate(ratings_testings_by_user, rcListBase)\n",
        "# Evaluation scores\n",
        "print(f'Rule1: \\n{round(scoreContent, 4)}')\n",
        "print(f'Base_case: \\n{round(scoreBase, 4)}')\n",
        "print(f'Improvemnt of Content-based method: \\n{round(100*(scoreContent-scoreBase)/scoreContent, 1)} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhUWZ2gjPRlR",
        "outputId": "7ae7678c-1b84-4442-b2d7-2b215ac78da0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rule1: \n",
            "0.1068\n",
            "Base_case: \n",
            "0.0983\n",
            "Improvemnt of Content-based method: \n",
            "7.9 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step5: Discussions of obstacles\n",
        "\n",
        "The reasons that we cannot generate significant score improvements are described as follows:\n",
        "*   Limiting testing users (38/584=6.5%) have purchase (comment) historical records.\n",
        "*   The average content-based score is really low, which reflects that the majority of products the user new bought are irrelated to its history purchased.\n",
        "\n"
      ],
      "metadata": {
        "id": "sED9o2in_Lqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "userWithHistory = []\n",
        "for user in users:\n",
        "    existHistory = metadata[metadata['asin'].isin(ratings_trainings[ratings_trainings['reviewerID'] == user]['asin'].tolist())]['asin'].tolist()\n",
        "    if len(existHistory) != 0:\n",
        "        userWithHistory.append(user)\n",
        "userWithHistoryAns = {}\n",
        "for i in range(len(userWithHistory)):\n",
        "  userWithHistoryAns[userWithHistory[i]] = ratings_testings_by_user[userWithHistory[i]]"
      ],
      "metadata": {
        "id": "CLrL1lqfZSPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rcListRule2 = recommender_content1(ratings_trainings, userWithHistory)\n",
        "# Evaluation scores\n",
        "print(f'Rule1: \\n{round(evaluate(userWithHistoryAns, rcListRule2), 4)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVDACTBHbZno",
        "outputId": "2cce361a-ffc7-4cc2-d490-d563f8947d1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rule1: \n",
            "0.0119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_item(item_input, k=2):\n",
        "    try:\n",
        "        item_index = mapping[item_input]\n",
        "        similarity_score = list(enumerate(similarity_matrix[item_index]))\n",
        "        similarity_score = sorted(similarity_score, key=lambda x: x[1], reverse=True)\n",
        "        similarity_score = similarity_score[:k]\n",
        "        item_indices = [i[0] for i in similarity_score]\n",
        "        return (metadata_titleOnly['asin'].iloc[item_indices].tolist())\n",
        "    except:\n",
        "        return []"
      ],
      "metadata": {
        "id": "SjdEfG54GtNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_item_lookup(item_input, item_ans):\n",
        "    try:\n",
        "        resultString = ''\n",
        "        item_index = mapping[item_input]\n",
        "        item_ans_index = mapping[item_ans]\n",
        "        similarity_score = list(enumerate(similarity_matrix[item_index]))\n",
        "        similarity_score = sorted(similarity_score, key=lambda x: x[1], reverse=True)\n",
        "        i_rank = 0\n",
        "        while (item_ans_index != similarity_score[i_rank][0]):\n",
        "            i_rank +=1\n",
        "            \n",
        "        resultString = 'Acual bought item is ranked as: '+str(i_rank)+' with score: '+str(round(similarity_score[i_rank][1],4))\n",
        "        return resultString\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "testUsers = ['A1UAOE8KO7Q1DZ', 'A1WEFBEJ7OHSVZ', 'A1WSZED2O5MA5T', 'A28E3FNV1BYC94', 'A29834GBB4DOP1']\n",
        "for user in testUsers:\n",
        "    asin2titleNew = []\n",
        "    actualtitleNew = []\n",
        "    existHistory = metadata[metadata['asin'].isin(ratings_trainings[ratings_trainings['reviewerID'] == user]['asin'].tolist())]['asin'].tolist()\n",
        "\n",
        "    for historyItem in existHistory:\n",
        "        asin2titleNew.append(metadata_titleOnly[metadata_titleOnly['asin']==historyItem]['title'].iloc[0])\n",
        "        realBoughtList = userWithHistoryAns[user]\n",
        "        for realBoughtItem in realBoughtList:\n",
        "            actualtitleNew.append(metadata_titleOnly[metadata_titleOnly['asin']==realBoughtItem]['title'].iloc[0])\n",
        "            resultStr = recommend_item_lookup(historyItem, realBoughtItem)\n",
        "    for j in asin2titleNew:\n",
        "        print('\\n'+user+' once bought: '+j)\n",
        "    for k in actualtitleNew:\n",
        "        print('\\n'+user+' now bought: '+k)\n",
        "    print('\\n'+resultStr)\n",
        "    print('------------------------------')"
      ],
      "metadata": {
        "id": "BgJ4mL7nF9pk",
        "outputId": "2b338853-7965-402e-d674-92aa9481eddd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "A1UAOE8KO7Q1DZ once bought: H2O+ Sea Salt Hydrating Body Lotion - 250ml/8.5oz\n",
            "\n",
            "A1UAOE8KO7Q1DZ now bought: Algenist Genius Ultimate Anti-Aging Eye Cream w/ Alguronic Acid (.17 oz) Mini Travel Size\n",
            "\n",
            "Acual bought item is ranked as: 29267 with score: 0.0\n",
            "------------------------------\n",
            "\n",
            "A1WEFBEJ7OHSVZ once bought:  Fairy Tales Tangle Tamer Static Free &nbsp;Detangling Spray for Kids - 12 oz\n",
            "\n",
            "A1WEFBEJ7OHSVZ now bought: Cantu Natural Hair Wave Whip Curling Mousse,8.4 oz\n",
            "\n",
            "Acual bought item is ranked as: 3261 with score: 0.0316\n",
            "------------------------------\n",
            "\n",
            "A1WSZED2O5MA5T once bought: Helen of Troy 1579 Tangle Free Hot Air Brush, White, 3/4 Inch Barrel\n",
            "\n",
            "A1WSZED2O5MA5T now bought: NICKA K Vivid Matte Lipstick NMS07 Black\n",
            "\n",
            "Acual bought item is ranked as: 25702 with score: 0.0\n",
            "------------------------------\n",
            "\n",
            "A28E3FNV1BYC94 once bought: Vibrant Bright Skin Lightening Cream by Healthy Vibes (2 oz) - Skin Whitening Treatment for Birthmarks, Blemishes, Pigmentation - Safe for Sensitive Areas - Natural Ingredients and Hydroquinone Free\n",
            "\n",
            "A28E3FNV1BYC94 now bought: Booty Magic | Butt Enhancement Cream - 2 Month Supply\n",
            "\n",
            "Acual bought item is ranked as: 3603 with score: 0.0286\n",
            "------------------------------\n",
            "\n",
            "A29834GBB4DOP1 once bought: Garnier Whole Blends Coconut Cocoa Butter Shampoo and Conditioner 12 ounces each\n",
            "\n",
            "A29834GBB4DOP1 now bought: Dream Cream Self Preserving 8.4oz\n",
            "\n",
            "Acual bought item is ranked as: 30917 with score: 0.0\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Examples of our testing users\n",
        "\n",
        "1.   Body Lotion vs. Anti-Aging Eye Cream<br>\n",
        "<img src='https://drive.google.com/uc?id=1xx-Grpx6Y3XyomJhtLVlzwre907abQRq' width=\"250\" height=\"175\"><br>\n",
        "2.   Detangling Spray vs. Hair Wave Whip Curling Mousse<br>\n",
        "<img src='https://drive.google.com/uc?id=1H6Pgz7wLa6S7RsSAQyEv95Z4JaBbGXSa' width=\"250\" height=\"175\"><br>\n",
        "3.   Tangle Free Hot Air Brush vs. Lipstick<br>\n",
        "<img src='https://drive.google.com/uc?id=1Tgyz2SCNcj_w3ZPohMXppLlfYQ21otOg' width=\"250\" height=\"175\"><br>\n",
        "4.   Skin Lightening Cream vs. Butt Enhancement Cream<br>\n",
        "<img src='https://drive.google.com/uc?id=1uUC0HGqW2mTSFSWmhSSezdodT9WSVr5e' width=\"250\" height=\"175\"><br>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1GdvsBz3BDgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "o4kG4T3wDB7a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}